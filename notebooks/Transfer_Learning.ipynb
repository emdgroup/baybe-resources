{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "MJUe",
   "metadata": {
    "marimo": {
     "config": {
      "hide_code": true
     }
    }
   },
   "source": [
    "# Transfer learning\n",
    "\n",
    "This file contains some examples regarding to the topic of **transfer learning**. It demonstrates how to use BayBE's transfer learning capabilities to improve the performance of campaigns if data from similar campaigns is available.\n",
    "\n",
    "**Note:**\n",
    "The term *transfer learning* is somewhat ambiguous, and different people might have different interpretations of what is meant by this term. We thus recommend to first read [the userguide on transfer learning](https://emdgroup.github.io/baybe/0.14.2/userguide/transfer_learning.html) to ensure that it is clear how to interpret this term in the context of BayBE.\n",
    "\n",
    "**Caution:**\n",
    "To really see the effects of transfer learning, it is necessary to run longer tests than the ones presented in this notebook. Since the goal of this notebook is to demonstrate how to use and set up transfer learning in BayBE, the results obtained by just executing this notebook might not be representative. We provide a pre-computed version in the `images` subfolder."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "084f2c24",
   "metadata": {},
   "source": [
    "## Installation\n",
    "\n",
    "To install `BayBE` in AWS SageMaker, make sure that you have the `conda_python3` kernel activated. Then, run the following cell to install all required packagages.\n",
    "Note that this might take some minutes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "87a5a7a0",
   "metadata": {},
   "outputs": [],
   "source": [
    "%mamba install h5py\n",
    "%pip install baybe[chem,simulation]==0.14.2 seaborn"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "vblA",
   "metadata": {
    "marimo": {
     "config": {
      "hide_code": true
     }
    }
   },
   "source": [
    "## Some basic settings and data loading\n",
    "\n",
    "We begin by introducing some settings that will be used later in this notebook. We also load the data, which is the a slightly adjusted version of the Shields-Dataset used in other notebooks, where the `Temperature` has been replaced by `Lab`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bkHC",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "data = pd.read_csv(\"../data/shields_tl.csv\")\n",
    "data"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "lEQa",
   "metadata": {
    "marimo": {
     "config": {
      "hide_code": true
     }
    }
   },
   "source": [
    "## Figuring out if transfer learning should be used at all\n",
    "\n",
    "The first step of a potential transfer learning workflow should always be a detailed analysis of the data. The reason is that transfer learning assume that there is some positive correlation in the data which can then be leveraged. If there is no such positive correlation, then transfer learning is not the correct tool to use, and attempting to use can even be harmful.\n",
    "\n",
    "In this example, we want to evaluate if we can use transfer learning for leveraging knowledge that we gained for reactions in two labs for the third lab. We thus begin by visualizing the data, using a small helper function that is defined in the next cell."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "PKri",
   "metadata": {
    "marimo": {
     "config": {
      "hide_code": true
     }
    }
   },
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "from pathlib import Path\n",
    "from collections import defaultdict\n",
    "import csv\n",
    "from itertools import combinations\n",
    "import scipy.stats as stats\n",
    "\n",
    "labs = [\"A\", \"B\", \"C\"]\n",
    "concentrations = [0.057, 0.1, 0.153]\n",
    "units = {\"lab\": \"\", \"concentration\": \"mol / l\"}\n",
    "parameters_to_analyze = {\"lab\": labs, \"concentration\": concentrations}\n",
    "\n",
    "def analyze_data(file_path: Path, parameter_to_analyze: str):\n",
    "    data = defaultdict(lambda: defaultdict(list))\n",
    "    with open(file_path, \"r\") as file:\n",
    "        csv_reader = csv.DictReader(file)\n",
    "        config = {}\n",
    "        for row in csv_reader:\n",
    "            config[\"base\"] = row[\"Base_Name\"]\n",
    "            config[\"ligand\"] = row[\"Ligand_Name\"]\n",
    "            config[\"solvent\"] = row[\"Solvent_Name\"]\n",
    "            config[\"concentration\"] = float(row[\"Concentration\"])\n",
    "            config[\"lab\"] = row[\"Lab\"]\n",
    "            yield_value = float(row[\"yield\"])\n",
    "\n",
    "            # config key consists of every key in the config dictionary, except for\n",
    "            # the parameter to analyze\n",
    "            config_key = tuple(\n",
    "                value\n",
    "                for key, value in config.items()\n",
    "                if key != parameter_to_analyze\n",
    "            )\n",
    "            data[config_key][config[parameter_to_analyze]].append(yield_value)\n",
    "\n",
    "    # form all possible combinations of the parameter to analyze\n",
    "    permutations = list(\n",
    "        combinations(parameters_to_analyze[parameter_to_analyze], 2)\n",
    "    )\n",
    "\n",
    "    for v1, v2 in permutations:\n",
    "        slopes = []\n",
    "\n",
    "        yields_v1: list[float] = []\n",
    "        yields_v2: list[float] = []\n",
    "\n",
    "        for parameter_config, parameter_combination in data.items():\n",
    "            if v1 in parameter_combination and v2 in parameter_combination:\n",
    "                yield_v1 = parameter_combination[v1][0]\n",
    "                yields_v1.append(yield_v1)\n",
    "                yield_v2 = parameter_combination[v2][0]\n",
    "                yields_v2.append(yield_v2)\n",
    "                if parameter_to_analyze != \"lab\":\n",
    "                    slope = (yield_v2 - yield_v1) / (v2 - v1)\n",
    "                    slopes.append(slope)\n",
    "\n",
    "        ## Correlation between yields at v1 and v2\n",
    "        corr, _ = stats.pearsonr(yields_v1, yields_v2)\n",
    "        unit_str = f\" {units[parameter_to_analyze]}\" if units[parameter_to_analyze] else \"\"\n",
    "        print(\n",
    "            f\"Pearson correlation coefficient (PCC) between yields at \"\n",
    "            f\"{v1}{unit_str} and \"\n",
    "            f\"{v2}{unit_str}: {corr:.2f}\"\n",
    "        )\n",
    "        scc, _ = stats.spearmanr(yields_v1, yields_v2)\n",
    "        print(\n",
    "            f\"Spearman rank correlation coefficient (SCC) between yields at \"\n",
    "            f\"{v1}{unit_str} and \"\n",
    "            f\"{v2}{unit_str}: {scc:.2f}\"\n",
    "        )\n",
    "        slope, intercept, r_value, p_value, std_err = stats.linregress(\n",
    "            yields_v1, yields_v2\n",
    "        )\n",
    "        print(f\"R2 value: {r_value**2:.2f}\")\n",
    "\n",
    "        # plot the correlation between the yields at v1 and v2\n",
    "        fig, ax = plt.subplots(figsize=(8, 6))\n",
    "        ax.scatter(\n",
    "            yields_v1,\n",
    "            yields_v2,\n",
    "            color=\"#0e69af\",\n",
    "            edgecolor=\"black\",\n",
    "            s=15,\n",
    "            alpha=0.8,\n",
    "            linewidth=0.5,\n",
    "        )\n",
    "        # include the regression line\n",
    "        x = [min(yields_v1), max(yields_v1)]\n",
    "        y = [slope * x_i + intercept for x_i in x]\n",
    "        ax.plot(\n",
    "            x,\n",
    "            y,\n",
    "            color=\"grey\",\n",
    "            label=f\"R$^2$ = {r_value**2:.2f}, PCC = {corr:.2f}, SCC = {scc:.2f}\",\n",
    "        )\n",
    "        ax.set_xlabel(f\"yield at {v1}{unit_str}\")\n",
    "        ax.set_ylabel(f\"yield at {v2}{unit_str}\")\n",
    "        ax.set_title(\n",
    "            f\"{parameter_to_analyze} combination: \"\n",
    "            f\"{v1}{unit_str} to \"\n",
    "            f\"{v2}{unit_str}\"\n",
    "        )\n",
    "        # include 1:1 line\n",
    "        ax.plot(x, x, color=\"black\", linestyle=\"--\", label=\"1:1 line\")\n",
    "        ax.grid(True)\n",
    "        # show the legend\n",
    "        ax.legend()\n",
    "        plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "Xref",
   "metadata": {
    "marimo": {
     "config": {
      "hide_code": true
     }
    }
   },
   "source": [
    "**Task:**\n",
    "The helper function is written in a generic way, allowing to also investigate other potential parameters. Instead of investigating whether the lab is a suitable candidate for transfer learning, investigate whether or not the concentration could also be used. Also think about why makes the concentration parameter inherently different from the lab parameter, and what the influence of this on the question of whether or not to use it as a `TaskParameter` is."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "SFPL",
   "metadata": {},
   "outputs": [],
   "source": [
    "analyze_data(Path(\"../data/shields_tl.csv\"), \"lab\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "BYtC",
   "metadata": {
    "marimo": {
     "config": {
      "hide_code": true
     }
    }
   },
   "source": [
    "Since the data is positively correlated, this is indeed a use case in which transfer learning can be used. We thus continue with describing how we can now setup BayBE to leverage this."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "RGSE",
   "metadata": {
    "marimo": {
     "config": {
      "hide_code": true
     }
    }
   },
   "source": [
    "## Setting up the BayBE campaign\n",
    "\n",
    "After we have analyzed the data and came to the conclusion that we want to approach this optimization with transfer learning, we now set up our `BayBE` campaign. We first begin by collecting all parts of the campaign that are not related to Transfer Learning.\n",
    "\n",
    "**Note:**\n",
    "This example uses substance encodings. In case you are interested in more details on them, check out the `ChemicalEncodings` example!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "Kclp",
   "metadata": {},
   "outputs": [],
   "source": [
    "from baybe.parameters import (\n",
    "    NumericalDiscreteParameter,\n",
    "    SubstanceParameter,\n",
    ")\n",
    "from baybe.targets import NumericalTarget\n",
    "from baybe.objectives import SingleTargetObjective\n",
    "from utils import create_dict_from_columns\n",
    "\n",
    "encoding = \"RDKIT2DDESCRIPTORS\"\n",
    "df = pd.read_csv(\"../data/shields_tl.csv\")\n",
    "\n",
    "substances = {\n",
    "    \"bases\": create_dict_from_columns(df, \"Base_Name\", \"Base_SMILES\"),\n",
    "    \"ligands\": create_dict_from_columns(df, \"Ligand_Name\", \"Ligand_SMILES\"),\n",
    "    \"solvents\": create_dict_from_columns(df, \"Solvent_Name\", \"Solvent_SMILES\"),\n",
    "}\n",
    "\n",
    "parameters = [\n",
    "    SubstanceParameter(\n",
    "        name=\"Solvent_Name\",\n",
    "        data=substances[\"solvents\"],\n",
    "        encoding=encoding,\n",
    "    ),\n",
    "    SubstanceParameter(\n",
    "        name=\"Base_Name\", data=substances[\"bases\"], encoding=encoding\n",
    "    ),\n",
    "    SubstanceParameter(\n",
    "        name=\"Ligand_Name\",\n",
    "        data=substances[\"ligands\"],\n",
    "        encoding=encoding,\n",
    "    ),\n",
    "    NumericalDiscreteParameter(\n",
    "        name=\"Concentration\", values=concentrations, tolerance=0.001\n",
    "    ),\n",
    "]\n",
    "\n",
    "objective = SingleTargetObjective(NumericalTarget(name=\"yield\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "emfo",
   "metadata": {
    "marimo": {
     "config": {
      "hide_code": true
     }
    }
   },
   "source": [
    "Transfer Learning in `BayBE` is enabled by using a special parameter - the [`TaskParameter`](https://emdgroup.github.io/baybe/0.14.2/userguide/transfer_learning.html#the-role-of-the-taskparameter). This parameter is used to \"mark\" the context of individual experiments, and thus to \"align\" different campaigns along their context dimension. The set of all possible contexts is provided upon the initialization of the `TaskParameter` by providing them as `values`.\n",
    "\n",
    "In this example, each lab corresponds to a different `context`. The set of `values` is thus the set of all labs. The `active_values` describes for which tasks recommendations should be given. This ensures that `BayBE` does not recommend to conduct experiments for a context that might no longer be available.\n",
    "\n",
    "We can then combine the `TaskParameters` together with the components defined above to create one campaign for each lab."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "Hstk",
   "metadata": {},
   "outputs": [],
   "source": [
    "from baybe.parameters import TaskParameter\n",
    "from baybe import Campaign\n",
    "from baybe.searchspace import SearchSpace\n",
    "\n",
    "tl_campaigns = {\n",
    "    lab: Campaign(\n",
    "        searchspace=SearchSpace.from_product(\n",
    "            parameters=parameters\n",
    "            + [\n",
    "                TaskParameter(\n",
    "                    name=\"Lab\",\n",
    "                    values=labs,\n",
    "                    active_values=[lab],\n",
    "                )\n",
    "            ]\n",
    "        ),\n",
    "        objective=objective,\n",
    "    )\n",
    "    for lab in labs\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "nWHF",
   "metadata": {
    "marimo": {
     "config": {
      "hide_code": true
     }
    }
   },
   "source": [
    "## Setting up the simulation\n",
    "\n",
    "We now set up the simulation loop. This requires us to define the number of DoE iterations as well as the number of Monte Carlo iterations.\n",
    "\n",
    "Since we want to investigate the influence of Transfer Learning, we will provide the campaigns with batches of initial data from the labs that are *not* being active for the corresponding campaign. The percentage of points sampled for this is given in the `SAMPLED_FRACTIONS` list. For each Monte Carlo iteration, we sample a different batch of initial data that is then being used by the algorithm. In addition, we compare the results to a \"baseline\" that is not using any Transfer Learning.\n",
    "\n",
    "**Caution:**\n",
    "Leveraging existing data requires a significant amount of data, in particular when we want to compare different amounts of sampled data. The execution can thus take quite some time if using settings that really show the effect.\n",
    "To really see the impact of Transfer Learning, you need to run the following code with more Monte Carlo Iterations, which might take quite some time. If you are interested in looking at some pre-computed results, have a look at the pre-computed image in the `images` subfolder."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "iLit",
   "metadata": {},
   "outputs": [],
   "source": [
    "from baybe.simulation import simulate_scenarios\n",
    "\n",
    "from baybe.utils.random import set_random_seed\n",
    "\n",
    "N_DOE_ITERATIONS = 2\n",
    "BATCH_SIZE = 2\n",
    "N_MC_ITERATIONS = 3\n",
    "set_random_seed(1337)\n",
    "\n",
    "SAMPLE_FRACTIONS = [0.01, 0.05, 0.1, 0.15]\n",
    "\n",
    "def optimize_for_lab(\n",
    "    lab: str,\n",
    "    tl_campaigns: dict[str, Campaign] = tl_campaigns,\n",
    "    data: pd.DataFrame = data,\n",
    "    sample_fractions: list[float] = SAMPLE_FRACTIONS\n",
    "):\n",
    "\n",
    "    lookup = data.copy(deep=True)\n",
    "\n",
    "    print(f\"\\n\\nLab: {lab}\")\n",
    "    excluded_labs = [l for l in labs if l != lab]\n",
    "\n",
    "    print(f\"Taking additional data from {excluded_labs} into account.\\n\")\n",
    "    campaign = tl_campaigns[lab]\n",
    "    # Lookup table that contains all data except the data for the current lab.\n",
    "    lookup_other_data = lookup[lookup[\"Lab\"] != lab].copy(deep=True)\n",
    "\n",
    "    results: list[pd.DataFrame] = []\n",
    "    for p in sample_fractions:\n",
    "        print(\"Percentage of data used: \", p)\n",
    "        result_fraction = simulate_scenarios(\n",
    "            {f\"{int(100 * p)}\": campaign},\n",
    "            lookup,\n",
    "            initial_data=[\n",
    "                lookup_other_data.sample(frac=p) for _ in range(N_MC_ITERATIONS)\n",
    "            ],\n",
    "            batch_size=BATCH_SIZE,\n",
    "            n_doe_iterations=N_DOE_ITERATIONS,\n",
    "        )\n",
    "        results.append(result_fraction)\n",
    "\n",
    "    print(\"Percentage of data used: 0.0\")\n",
    "    result_baseline = simulate_scenarios(\n",
    "        {\"0\": campaign},\n",
    "        lookup,\n",
    "        batch_size=BATCH_SIZE,\n",
    "        n_doe_iterations=N_DOE_ITERATIONS,\n",
    "        n_mc_iterations=N_MC_ITERATIONS,\n",
    "    )\n",
    "    results = pd.concat([result_baseline, *results])\n",
    "\n",
    "    results.rename(\n",
    "        columns={\n",
    "            \"Scenario\": \"% of data used\",\n",
    "            \"Num_Experiments\": \"Number of experiments\",\n",
    "            \"yield_CumBest\": \"Running best yield\",\n",
    "        },\n",
    "        inplace=True,\n",
    "    )\n",
    "\n",
    "    return results"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ZHCJ",
   "metadata": {
    "marimo": {
     "config": {
      "hide_code": true
     }
    }
   },
   "source": [
    "We now finally run the simulation code and investigate the results. The variable `lab_to_investigate` can be changed to any of the available labs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ROlb",
   "metadata": {},
   "outputs": [],
   "source": [
    "from utils import backtest_plot\n",
    "\n",
    "LAB_TO_INVESTIGATE = \"B\"\n",
    "\n",
    "backtest_plot(\n",
    "    df=optimize_for_lab(LAB_TO_INVESTIGATE),\n",
    "    x=\"Number of experiments\",\n",
    "    y=\"Running best yield\",\n",
    "    hue=\"% of data used\",\n",
    ")\n",
    "plt.gcf()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "qnkX",
   "metadata": {
    "marimo": {
     "config": {
      "hide_code": true
     }
    }
   },
   "source": [
    "The following image shows the pre-computed result that was obtained with the following settings:\n",
    "* `SAMPLED_FRACTIONS = [0.01, 0.05, 0.1, 0.15]`\n",
    "* `N_DOE_ITERATIONS = 30`\n",
    "* `BATCH_SIZE = 2`\n",
    "* `N_MC_ITERATIONS = 40`\n",
    "* `LAB_TO_INVESTIGATE = \"B\"`\n",
    "\n",
    "It demonstrates that already a little data can help in the beginning of an experimental campaign, and that although more data is more favourable, the effects are diminishing at some point."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "115b239e",
   "metadata": {},
   "source": [
    "![Precomputed example](../images/transfer_learning_precomputed.png)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "baybe-resources (3.13.3)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.3"
  },
  "marimo": {
   "app_config": {
    "app_title": "Transfer Learning",
    "width": "medium"
   },
   "marimo_version": "0.19.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
